import pycuda.driver as cuda
import pycuda.autoinit
from pycuda.compiler import SourceModule

import numpy as np
import pandas as pd

df = pd.read_csv("data.csv")

# Definição do Kernel CUDA
def evaluate_line(line) :
    for u in ["sinf", "cosf", "tanf", "sqrtf", "expf"]:
        line = line.replace(u, f"np.{u[:-1]}")
    for c in df.columns:
        line = line.replace(f"_{c}_", f"(df[\"{c}\"].values)")

    a = eval(line)
    b = df["y"]
    return np.square(np.subtract(a, b)).mean()

kernel_code = """
#include <stdio.h>

__global__ void score_kernel(char* function, float* result, int N) {
    int idx = threadIdx.x + blockIdx.x * blockDim.x;
    if (idx < N) {
        result[idx] = {evaluate_line(function)};
    }
}

"""
    


# Compilação do Kernel CUDA
score_kernel = SourceModule(kernel_code).get_function("score_kernel")

funs = [ line.strip() for line in open("functions.txt").readlines() ]

def parallel_score(function):

    N = len(df)

    # criar input e output
    host = function
    dev = np.zeros(N, np.float32) # output array com a media de todas as funções
    # faço o np.square(np.subtract(a, b)).mean() para cada um no gpu?

    # Alocar memória na GPU para o host e para o dev
    host_gpu = cuda.mem_alloc(len(host)) #alocar o tamanho do input na memoria da gpu # 4 é o tamanho em bytes para float
    dev_result = cuda.mem_alloc(N * np.float32) #alocar o tamanho do array output na memoria da gpu # 4 é o tamanho em bytes para float
    output = np.empty(N, dtype=np.float32)

    # Transferir dados para a GPU
    cuda.memcpy_htod(host_gpu, host)

    #cuda.memcpy_htod()
    #d_data = cuda.to_device(data.astype(np.float32))
    #line = cuda.to_device(l.encode())
    #line = cuda.to_device(output.encode())
    #d_y = cuda.to_device(y.astype(np.float32))

    block_size = 1024
    #grid_size = N // block_size
    grid_size = (N + block_size - 1) // block_size

    # Executar o kernel na GPU
    #score_kernel(d_result, d_data, block=(N, 1, 1), grid=(1, 1))
    score_kernel(host_gpu, dev_result, block=(block_size, 1, 1), grid=(grid_size, 1))

    # Transferir resultados da GPU de volta para a CPU
    cuda.memcpy_dtoh(output, dev_result)

    # Libertar recursos na GPU
    host_gpu.free()
    dev_result.free()

    return output

l = funs[0]
print(parallel_score(l) , l)
r = min([ (parallel_score(line), line) for line in funs ])
print(f"{r[0]} {r[1]}")
